{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30716,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Sonny","metadata":{}},{"cell_type":"markdown","source":"t5-small 128\nMarianMTModel - 64","metadata":{}},{"cell_type":"code","source":"import os\nfrom accelerate.utils import write_basic_config\nwrite_basic_config() # Write a config file\nos._exit(00) # Restart the notebook","metadata":{"execution":{"iopub.status.busy":"2024-06-02T01:56:06.389179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install sacremoses -q","metadata":{"execution":{"iopub.status.busy":"2024-06-02T01:44:26.629890Z","iopub.execute_input":"2024-06-02T01:44:26.630577Z","iopub.status.idle":"2024-06-02T01:44:39.940335Z","shell.execute_reply.started":"2024-06-02T01:44:26.630541Z","shell.execute_reply":"2024-06-02T01:44:39.939412Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Error parsing requirements for aiohttp: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install sacrebleu -q","metadata":{"execution":{"iopub.status.busy":"2024-06-02T03:54:24.174069Z","iopub.execute_input":"2024-06-02T03:54:24.174447Z","iopub.status.idle":"2024-06-02T03:54:36.901293Z","shell.execute_reply.started":"2024-06-02T03:54:24.174416Z","shell.execute_reply":"2024-06-02T03:54:36.900155Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[33mWARNING: Error parsing requirements for aiohttp: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict, load_metric\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, DataCollatorForSeq2Seq, get_scheduler\nfrom transformers import MarianMTModel, MarianTokenizer\n\nfrom torch.utils.data import DataLoader\nfrom accelerate import Accelerator, notebook_launcher\nfrom accelerate.utils import set_seed\nfrom torch.optim import AdamW\nfrom time import time\nimport torch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset('Helsinki-NLP/opus_books', 'en-hu')","metadata":{"execution":{"iopub.status.busy":"2024-06-02T01:56:16.592328Z","iopub.execute_input":"2024-06-02T01:56:16.592839Z","iopub.status.idle":"2024-06-02T01:56:18.157705Z","shell.execute_reply.started":"2024-06-02T01:56:16.592810Z","shell.execute_reply":"2024-06-02T01:56:18.156764Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"dataset['train'][0]","metadata":{"execution":{"iopub.status.busy":"2024-06-02T01:56:18.158974Z","iopub.execute_input":"2024-06-02T01:56:18.159334Z","iopub.status.idle":"2024-06-02T01:56:18.167299Z","shell.execute_reply.started":"2024-06-02T01:56:18.159300Z","shell.execute_reply":"2024-06-02T01:56:18.166264Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"{'id': '0',\n 'translation': {'en': 'Source: Project GutenbergAudiobook available here',\n  'hu': 'Source: mek.oszk.huTranslation: Szenczi MiklósAudiobook available here'}}"},"metadata":{}}]},{"cell_type":"code","source":"val_test_set = dataset['train'].train_test_split(test_size=0.2, seed=42)\ntest_set = val_test_set['test'].train_test_split(test_size=0.5, seed=42)\n\ndataset = DatasetDict({\n    'train': val_test_set['train'],\n    'val': test_set['test'],\n    'test': test_set['train']\n})","metadata":{"execution":{"iopub.status.busy":"2024-06-02T01:56:18.169333Z","iopub.execute_input":"2024-06-02T01:56:18.169572Z","iopub.status.idle":"2024-06-02T01:56:18.184545Z","shell.execute_reply.started":"2024-06-02T01:56:18.169551Z","shell.execute_reply":"2024-06-02T01:56:18.183783Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-06-02T01:56:18.185766Z","iopub.execute_input":"2024-06-02T01:56:18.186353Z","iopub.status.idle":"2024-06-02T01:56:18.192988Z","shell.execute_reply.started":"2024-06-02T01:56:18.186321Z","shell.execute_reply":"2024-06-02T01:56:18.192071Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'translation'],\n        num_rows: 109720\n    })\n    val: Dataset({\n        features: ['id', 'translation'],\n        num_rows: 13716\n    })\n    test: Dataset({\n        features: ['id', 'translation'],\n        num_rows: 13715\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"def get_config():\n    config = {\n        'model_name': 'Helsinki-NLP/opus-mt-en-hu', # 't5-small', \n        'max_length': 128,\n        'batch_size': 64,\n        'lr': 10 ** -5,\n        'epochs': 10,\n        'seed': 42,\n        'metric_name': 'sacrebleu',\n        'save_model': '/kaggle/working/model.pth',\n        \n    }\n    return config","metadata":{"execution":{"iopub.status.busy":"2024-06-02T01:56:18.194076Z","iopub.execute_input":"2024-06-02T01:56:18.194397Z","iopub.status.idle":"2024-06-02T01:56:18.200531Z","shell.execute_reply.started":"2024-06-02T01:56:18.194372Z","shell.execute_reply":"2024-06-02T01:56:18.199667Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"config = get_config()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T01:56:29.126746Z","iopub.execute_input":"2024-06-02T01:56:29.127108Z","iopub.status.idle":"2024-06-02T01:56:29.131387Z","shell.execute_reply.started":"2024-06-02T01:56:29.127079Z","shell.execute_reply":"2024-06-02T01:56:29.130448Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### DataLoader","metadata":{}},{"cell_type":"code","source":"def get_dataloader(dataset, config, is_train):\n    if is_train == True:\n        return DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)\n    else:\n        return DataLoader(dataset, batch_size=config['batch_size'], drop_last=False, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T01:56:30.936232Z","iopub.execute_input":"2024-06-02T01:56:30.937246Z","iopub.status.idle":"2024-06-02T01:56:30.942391Z","shell.execute_reply.started":"2024-06-02T01:56:30.937198Z","shell.execute_reply":"2024-06-02T01:56:30.941510Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_loader = get_dataloader(dataset['train'], config, True)\nval_loader = get_dataloader(dataset['val'], config, False)\ntest_loader = get_dataloader(dataset['test'], config, False)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T01:36:29.927472Z","iopub.execute_input":"2024-06-02T01:36:29.927757Z","iopub.status.idle":"2024-06-02T01:36:29.934344Z","shell.execute_reply.started":"2024-06-02T01:36:29.927734Z","shell.execute_reply":"2024-06-02T01:36:29.933446Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"import wandb\n\n# Replace 'your-api-key' with your actual wandb API key\nwandb.login(key='06cc95a1f4faf48400aa0bf5e162b3ace6237a45')","metadata":{"execution":{"iopub.status.busy":"2024-06-02T01:56:33.505123Z","iopub.execute_input":"2024-06-02T01:56:33.506010Z","iopub.status.idle":"2024-06-02T01:56:35.560382Z","shell.execute_reply.started":"2024-06-02T01:56:33.505974Z","shell.execute_reply":"2024-06-02T01:56:35.559530Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msonnyinkai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"def train():\n    set_seed(config['seed'])\n    accelerator = Accelerator(mixed_precision='fp16', log_with='wandb')\n\n    # Initialize tokenizer and model\n    tokenizer = MarianTokenizer.from_pretrained(config['model_name'])\n    model = MarianMTModel.from_pretrained(config['model_name'])\n\n    # Initialize dataloaders\n    train_loader = get_dataloader(dataset['train'], config, True)\n    val_loader = get_dataloader(dataset['val'], config, False)\n\n    # Initialize optimizer\n    optimizer = AdamW(model.parameters(), lr=config['lr'])\n\n    # Initialize lr scheduler\n    num_training_steps = len(train_loader) * config['epochs']\n    lr_scheduler = get_scheduler('cosine', optimizer=optimizer, num_training_steps=num_training_steps, num_warmup_steps=0)\n\n    # Prepare with accelerator\n    model, optimizer, train_loader, val_loader, lr_scheduler = accelerator.prepare(model, optimizer, train_loader, val_loader, lr_scheduler)\n\n    accelerator.init_trackers('Machine Translation')\n\n    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    accelerator.print(f'Number of training parameters: {n_parameters}')\n    accelerator.print(f\"Batch size: {config['batch_size']}\")\n    start_time = time()\n    accelerator.print('======== Start training ======== ')\n    \n    # Training loop\n    for epoch in range(config['epochs']):\n        model.train()\n        training_loss = 0\n        for data in train_loader:\n            optimizer.zero_grad()\n            src = tokenizer(data['translation']['en'], max_length=config['max_length'], truncation=True, padding='longest', return_tensors='pt').to(accelerator.device)\n            labels = tokenizer(data['translation']['hu'], max_length=config['max_length'], truncation=True, padding='longest', return_tensors='pt')\n\n            outputs = model(**src, labels=labels.input_ids.to(accelerator.device))\n            loss = outputs.loss\n\n            accelerator.backward(loss)\n            optimizer.step()\n            \n            lr_scheduler.step()\n            optimizer.zero_grad()\n\n            training_loss += loss.item()\n            \n            accelerator.log({\"Training loss\": training_loss})\n\n        training_loss = training_loss / len(train_loader)\n        accelerator.log({\"Training loss\": training_loss})\n\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for data in val_loader:\n                src = tokenizer(data['translation']['en'], max_length=config['max_length'], truncation=True, padding='longest', return_tensors='pt').to(accelerator.device)\n                labels = tokenizer(data['translation']['hu'], max_length=config['max_length'], truncation=True, padding='longest', return_tensors='pt')\n\n                outputs = model(**src, labels=labels.input_ids.to(accelerator.device))\n                loss = outputs.loss\n\n                val_loss += loss.item()\n\n        val_loss = val_loss / len(val_loader)\n        accelerator.log({\"Val loss\": val_loss})\n        accelerator.print(f\"Epoch {epoch + 1}: Training loss = {training_loss}, Val loss = {val_loss}\")\n\n    accelerator.print('======== End training ========')\n    total_training_time = time() - start_time\n    accelerator.print(f\"Total training time: {total_training_time}\")\n\n    # Save model\n    accelerator.wait_for_everyone()\n    model = accelerator.unwrap_model(model)\n    accelerator.save(model, config['save_model'])\n    accelerator.end_training()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T02:03:16.486403Z","iopub.execute_input":"2024-06-02T02:03:16.486776Z","iopub.status.idle":"2024-06-02T02:03:16.510285Z","shell.execute_reply.started":"2024-06-02T02:03:16.486746Z","shell.execute_reply":"2024-06-02T02:03:16.509110Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"","metadata":{"execution":{"iopub.status.busy":"2024-06-02T02:03:14.588345Z","iopub.execute_input":"2024-06-02T02:03:14.589293Z","iopub.status.idle":"2024-06-02T02:03:14.596274Z","shell.execute_reply.started":"2024-06-02T02:03:14.589245Z","shell.execute_reply":"2024-06-02T02:03:14.595367Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"notebook_launcher(train, num_processes=2)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T02:04:12.919215Z","iopub.execute_input":"2024-06-02T02:04:12.919578Z","iopub.status.idle":"2024-06-02T03:28:33.537115Z","shell.execute_reply.started":"2024-06-02T02:04:12.919552Z","shell.execute_reply":"2024-06-02T03:28:33.535858Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Launching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240602_020418-knpavko5</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/sonnyinkai/Machine%20Translation/runs/knpavko5' target=\"_blank\">quiet-sky-17</a></strong> to <a href='https://wandb.ai/sonnyinkai/Machine%20Translation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/sonnyinkai/Machine%20Translation' target=\"_blank\">https://wandb.ai/sonnyinkai/Machine%20Translation</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/sonnyinkai/Machine%20Translation/runs/knpavko5' target=\"_blank\">https://wandb.ai/sonnyinkai/Machine%20Translation/runs/knpavko5</a>"},"metadata":{}},{"name":"stdout","text":"Number of training parameters: 76149760\nBatch size: 64\n======== Start training ======== \nEpoch 1: Training loss = 1.7197015921135883, Val loss = 1.2082806748372537\nEpoch 2: Training loss = 1.1870478390952646, Val loss = 1.0266323757392388\nEpoch 3: Training loss = 1.0526310902236502, Val loss = 0.9404577066500982\nEpoch 4: Training loss = 0.9819862646656436, Val loss = 0.890795718188639\nEpoch 5: Training loss = 0.9354820445284143, Val loss = 0.8604172170162201\nEpoch 6: Training loss = 0.9096408127904771, Val loss = 0.8410754987487087\nEpoch 7: Training loss = 0.8874246104336008, Val loss = 0.8301579163030341\nEpoch 8: Training loss = 0.8770192352347163, Val loss = 0.8242287288109461\nEpoch 9: Training loss = 0.8730430450889614, Val loss = 0.8222033762269549\nEpoch 10: Training loss = 0.8676336767512324, Val loss = 0.8218545957847878\n======== End training ========\nTotal training time: 5034.0282661914825\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.018 MB uploaded\\r'), FloatProgress(value=0.07578827073918239, max=1.…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>Val loss</td><td>█▅▃▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training loss</td><td>0.86763</td></tr><tr><td>Val loss</td><td>0.82185</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">quiet-sky-17</strong> at: <a href='https://wandb.ai/sonnyinkai/Machine%20Translation/runs/knpavko5' target=\"_blank\">https://wandb.ai/sonnyinkai/Machine%20Translation/runs/knpavko5</a><br/> View project at: <a href='https://wandb.ai/sonnyinkai/Machine%20Translation' target=\"_blank\">https://wandb.ai/sonnyinkai/Machine%20Translation</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240602_020418-knpavko5/logs</code>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Evaluation","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-06-02T03:48:50.277332Z","iopub.execute_input":"2024-06-02T03:48:50.277699Z","iopub.status.idle":"2024-06-02T03:48:50.282484Z","shell.execute_reply.started":"2024-06-02T03:48:50.277669Z","shell.execute_reply":"2024-06-02T03:48:50.281437Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"model = torch.load('/kaggle/working/model.pth', map_location='cpu')","metadata":{"execution":{"iopub.status.busy":"2024-06-02T03:48:24.652904Z","iopub.execute_input":"2024-06-02T03:48:24.653249Z","iopub.status.idle":"2024-06-02T03:48:24.880221Z","shell.execute_reply.started":"2024-06-02T03:48:24.653203Z","shell.execute_reply":"2024-06-02T03:48:24.879453Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"metric = load_metric(\"sacrebleu\")","metadata":{"execution":{"iopub.status.busy":"2024-06-02T03:54:46.440508Z","iopub.execute_input":"2024-06-02T03:54:46.440885Z","iopub.status.idle":"2024-06-02T03:54:46.856997Z","shell.execute_reply.started":"2024-06-02T03:54:46.440853Z","shell.execute_reply":"2024-06-02T03:54:46.856275Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/sacrebleu/sacrebleu.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"test_loader = get_dataloader(dataset['test'], config, False)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T03:54:59.323321Z","iopub.execute_input":"2024-06-02T03:54:59.324782Z","iopub.status.idle":"2024-06-02T03:54:59.329403Z","shell.execute_reply.started":"2024-06-02T03:54:59.324739Z","shell.execute_reply":"2024-06-02T03:54:59.328450Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"tokenizer = MarianTokenizer.from_pretrained(config['model_name'])","metadata":{"execution":{"iopub.status.busy":"2024-06-02T03:55:01.441998Z","iopub.execute_input":"2024-06-02T03:55:01.442665Z","iopub.status.idle":"2024-06-02T03:55:02.151003Z","shell.execute_reply.started":"2024-06-02T03:55:01.442630Z","shell.execute_reply":"2024-06-02T03:55:02.150109Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, tokenizer, test_loader, metric, device):\n    test_loss = 0 \n    bleu_score = 0 \n\n    model.to(device)\n    model.eval()\n    \n    with torch.no_grad():\n        for data in test_loader:\n            # Tokenize the input and labels\n            src = tokenizer(data['translation']['en'], max_length=config['max_length'], truncation=True, padding='longest', return_tensors='pt').to(device)\n            labels = tokenizer(data['translation']['hu'], max_length=config['max_length'], truncation=True, padding='longest', return_tensors='pt').to(device)\n            \n            # Forward pass\n            outputs = model(**src, labels=labels.input_ids)\n            loss = outputs.loss\n\n            test_loss += loss.item()\n\n            # Generate predictions\n            predictions = model.generate(input_ids=src.input_ids, attention_mask=src.attention_mask)\n            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n            decoded_labels = tokenizer.batch_decode(labels.input_ids, skip_special_tokens=True)\n\n            # Calculate BLEU score\n            decoded_labels = [[label] for label in decoded_labels]\n            result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n            bleu_score += result['score']\n\n    # Calculate average loss and BLEU score\n    test_loss = test_loss / len(test_loader)\n    bleu_score = bleu_score / len(test_loader)\n    print(f\"Test loss is: {test_loss} and BLEU score is: {bleu_score}\")\n\n    return test_loss, bleu_score\n","metadata":{"execution":{"iopub.status.busy":"2024-06-02T04:18:28.205366Z","iopub.execute_input":"2024-06-02T04:18:28.205775Z","iopub.status.idle":"2024-06-02T04:18:28.215590Z","shell.execute_reply.started":"2024-06-02T04:18:28.205748Z","shell.execute_reply":"2024-06-02T04:18:28.214598Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"evaluate(model, tokenizer, test_loader, metric, device)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T04:18:28.428941Z","iopub.execute_input":"2024-06-02T04:18:28.429254Z","iopub.status.idle":"2024-06-02T04:48:06.964841Z","shell.execute_reply.started":"2024-06-02T04:18:28.429215Z","shell.execute_reply":"2024-06-02T04:48:06.963732Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Test loss is: 0.8217921686726947 and BLEU score is: 9.359375810821396\n","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"(0.8217921686726947, 9.359375810821396)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}